#include "mobilenet_ssd_detector.h" 

#include "glog/logging.h"
#include "gflags/gflags.h"

namespace OPENVINO {

	MobilenetSSDDetector::MobilenetSSDDetector() {
			options_mobilenet_ssd_ = OptionsMobilenetSSD();
	}

	MobilenetSSDDetector::~MobilenetSSDDetector() {
	}

	int MobilenetSSDDetector::init(const std::string model_path) {
		LOG(INFO) << "start init. " << std::endl;

		// 1. Load inference engine
		LOG(INFO) << "Loading Inference Engine";
		InferenceEngine::Core ie;
		ie.GetVersions(options_mobilenet_ssd_.options_openvino_.device_name);

		std::map<std::string, std::map<std::string, std::string>> config;
		config[options_mobilenet_ssd_.options_openvino_.device_name] = {};
		std::map<std::string, std::string>& device_config = config.at(options_mobilenet_ssd_.options_openvino_.device_name);
		device_config[CONFIG_KEY(CPU_THREADS_NUM)] = std::to_string(options_mobilenet_ssd_.options_openvino_.nthreads);

		for (auto&& item : config) {		
			ie.SetConfig(item.second, item.first);	
		}	

		// 2. Read IR Generated by ModelOptimizer (.xml and .bin files)
		std::string binFileName = model_path.substr(0, model_path.rfind('.')) + ".bin";
		LOG(INFO) << "Loading network files: \n\t" << model_path << "\n\t" << binFileName;
		network_ = ie.ReadNetwork(model_path.c_str());
		LOG(INFO) << "Batch size is " << std::to_string(network_.getBatchSize());
		
		// 3. Prepare input blobs
		LOG(INFO) << "Preparing input blobs";
		InferenceEngine::InputsDataMap input_info(network_.getInputsInfo());
		for (auto & item : input_info) {
			auto input_data = item.second;
			input_data->setPrecision(InferenceEngine::Precision::U8);
			input_data->setLayout(InferenceEngine::Layout::NCHW);
		}
		input_info_ = input_info.begin()->second;
		input_name_ = input_info.begin()->first;

		// 4. Prepare output blobs 
		LOG(INFO) << "Preparing output blobs";
		InferenceEngine::OutputsDataMap output_info(network_.getOutputsInfo());
		output_info_ = output_info.begin()->second;
		output_name_ = output_info_->getName();	
		output_info_->setPrecision(InferenceEngine::Precision::FP32);
		CHECK_NOTNULL(output_info_);

		const InferenceEngine::SizeVector outputDims = output_info_->getTensorDesc().getDims();
		const int objectSize = outputDims[3];

		if (objectSize != 7) {
			throw std::logic_error("Output item should have 7 as a last dimension");
		}

		if (outputDims.size() != 4) {
			throw std::logic_error("Incorrect output dimensions for SSD model");
		}

		// 5. Loading model to the device 	
		LOG(INFO) << "Loading model to the device";
		InferenceEngine::ExecutableNetwork executable_network = ie.LoadNetwork(
				network_, 
				options_mobilenet_ssd_.options_openvino_.device_name);
		
		// 6. Create infer request 
		LOG(INFO) << "Create infer request";
		infer_request_ = executable_network.CreateInferRequest();	

		LOG(INFO) << "end init. ";
		return 0;
	}

	int MobilenetSSDDetector::detect(const cv::Mat& img_src, std::vector<ObjectInformation>* objects) {
		LOG(INFO) << "start detect.";

		// 1. Prepare input
		int width = img_src.cols;
		int height = img_src.rows;

		// resize image
		cv::Mat img_resized(img_src);
		cv::resize(img_src, img_resized, cv::Size(input_info_->getTensorDesc().getDims()[3], input_info_->getTensorDesc().getDims()[2]));

		std::shared_ptr<unsigned char> img_resized_data;
		int img_resized_size = img_resized.cols * img_resized.rows * img_resized.channels();
		img_resized_data.reset(new unsigned char[img_resized_size], std::default_delete<unsigned char[]>());
		for (int id = 0; id < img_resized_size; ++id) {
				img_resized_data.get()[id] = img_resized.data[id];
		}

		// Creating input blob 
		InferenceEngine::Blob::Ptr imageInput = infer_request_.GetBlob(input_name_);
		// Filling input tensor with images. First b channel, then g and r channels 
		InferenceEngine::MemoryBlob::Ptr mimage = InferenceEngine::as<InferenceEngine::MemoryBlob>(imageInput);
		if (!mimage) {
				LOG(ERROR) << "We expect image blob to be inherited from MemoryBlob, but by fact we were not able "
						"to cast imageInput to MemoryBlob";
				return -1;
		}
		// locked memory holder should be alive all time while access to its buffer happens
		auto minputHolder = mimage->wmap();
		size_t num_channels = mimage->getTensorDesc().getDims()[1];
		size_t image_size = mimage->getTensorDesc().getDims()[3] * mimage->getTensorDesc().getDims()[2];
		unsigned char *data = minputHolder.as<unsigned char *>();

		/** Iterate over all pixel in image (b,g,r) **/
		for (size_t pid = 0; pid < image_size; pid++) {
				/** Iterate over all channels **/
				for (size_t ch = 0; ch < num_channels; ++ch) {
						/**          [images stride + channels stride + pixel id ] all in bytes            **/
						data[ch * image_size + pid] = img_resized_data.get()[pid * num_channels + ch];
				}
		}

		// 2. Do inference 	
		LOG(INFO) << "Start inference";
		infer_request_.Infer();	

		// 3. Process output
		LOG(INFO) << "Processing output blobs";
		const InferenceEngine::Blob::Ptr output_blob = infer_request_.GetBlob(output_name_);
		InferenceEngine::MemoryBlob::CPtr moutput = InferenceEngine::as<InferenceEngine::MemoryBlob>(output_blob);
		if (!moutput) {
				throw std::logic_error("We expect output to be inherited from MemoryBlob, "
																"but by fact we were not able to cast output to MemoryBlob");
		}

		// locked memory holder should be alive all time while access to its buffer happens
		auto moutputHolder = moutput->rmap();
		const float *detection = moutputHolder.as<const InferenceEngine::PrecisionTrait<InferenceEngine::Precision::FP32>::value_type *>();

		std::vector<std::vector<int> > boxes(network_.getBatchSize());
		std::vector<std::vector<int> > classes(network_.getBatchSize());

		const InferenceEngine::SizeVector outputDims = output_info_->getTensorDesc().getDims();	
		const int maxProposalCount = outputDims[2];
		const int objectSize = outputDims[3];

		/* Each detection has image_id that denotes processed image */
		for (int curProposal = 0; curProposal < maxProposalCount; curProposal++) {

			float confidence = detection[curProposal * objectSize + 2];
			auto label = static_cast<int>(detection[curProposal * objectSize + 1]);
			auto xmin = static_cast<int>(detection[curProposal * objectSize + 3] * width);
			auto ymin = static_cast<int>(detection[curProposal * objectSize + 4] * height);
			auto xmax = static_cast<int>(detection[curProposal * objectSize + 5] * width);
			auto ymax = static_cast<int>(detection[curProposal * objectSize + 6] * height);

			if (confidence > options_mobilenet_ssd_.threshold) {
				/** Drawing only objects with >50% probability **/
				ObjectInformation object;
				object.name_ = options_mobilenet_ssd_.class_names[label];
				object.score_ = confidence;
				object.location_.x = xmin;
				object.location_.y = ymin;
				object.location_.width = xmax - xmin;
				object.location_.height = ymax - ymin;
    		objects->push_back(object);
			}
			std::cout << std::endl;
		}

		LOG(INFO) << "end detect.";
		return 0;
	}

}