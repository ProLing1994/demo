#include "inference_detection_openvino.hpp" 

#include "glog/logging.h"
#include "gflags/gflags.h"

#ifdef WIN32
#ifndef AIP_API
#define AIP_API 
#endif // AIP_API
#else
#ifndef AIP_API
#define AIP_API __attribute__((visibility("default")))
#endif // AIP_API
#endif

namespace inference_openvino {

  AIP_API rmInferenceDetectionOpenvino::rmInferenceDetectionOpenvino(const std::string strModelPath) {
	m_InferenceOptions = INFERENCE_OPTIONS_S();
	m_strModelPath = strModelPath;
  }

  AIP_API rmInferenceDetectionOpenvino::rmInferenceDetectionOpenvino(const std::string strModelPath, const INFERENCE_OPTIONS_S& InferenceOptions) {
	m_InferenceOptions = INFERENCE_OPTIONS_S(InferenceOptions);
	m_strModelPath = strModelPath;
  }

  AIP_API rmInferenceDetectionOpenvino::~rmInferenceDetectionOpenvino() {
  }

  AIP_API int rmInferenceDetectionOpenvino::Init() {
	LOG(INFO) << "Start init.";

	// 0. Check input
	if (m_InferenceOptions.OpenvinoOptions.strDeviceType != "CPU" &&
	  m_InferenceOptions.OpenvinoOptions.strDeviceType != "GPU" &&
	  m_InferenceOptions.OpenvinoOptions.strDeviceType != "MULTI:CPU,GPU") {
	  LOG(ERROR) << "ERROR, func: " << __FUNCTION__ << ", line: " << __LINE__
		<< ", DeviceName only support for ['CPU'/'GPU'/'MULTI:CPU,GPU'], can not be "
		<< m_InferenceOptions.OpenvinoOptions.strDeviceType;
	  return -1;
	}

	if (m_InferenceOptions.OpenvinoOptions.strDeviceType == "CPU" &&
	  (m_InferenceOptions.OpenvinoOptions.u32ThreadNum <= 0 ||
		m_InferenceOptions.OpenvinoOptions.u32ThreadNum > 32)) {
	  LOG(ERROR) << "ERROR, func: " << __FUNCTION__ << ", line: " << __LINE__
		<< ", ThreadNum only support for 1 ~ 32, can not be " << m_InferenceOptions.OpenvinoOptions.u32ThreadNum;
	  return -1;
	}

	if (m_strModelPath.find(".xml") == m_strModelPath.npos) {
	  LOG(ERROR) << "ERROR, func: " << __FUNCTION__ << ", line: " << __LINE__
		<< ", ModelPath type must be .xml";
	  return -1;
	}

	// 1. Load inference engine
	LOG(INFO) << "Loading inference engine";
	InferenceEngine::Core Ie;
	Ie.GetVersions(m_InferenceOptions.OpenvinoOptions.strDeviceType);
	LOG(INFO) << "Device Type: \n\t" << m_InferenceOptions.OpenvinoOptions.strDeviceType;

	if (m_InferenceOptions.OpenvinoOptions.strDeviceType == "CPU") {
	  std::map<std::string, std::map<std::string, std::string>> nConfig;
	  nConfig[m_InferenceOptions.OpenvinoOptions.strDeviceType] = {};
	  std::map<std::string, std::string>& nDeviceConfig = nConfig.at(m_InferenceOptions.OpenvinoOptions.strDeviceType);
	  nDeviceConfig[CONFIG_KEY(CPU_THREADS_NUM)] = std::to_string(m_InferenceOptions.OpenvinoOptions.u32ThreadNum);
	  LOG(INFO) << "CPU Thread Number: \n\t" << m_InferenceOptions.OpenvinoOptions.u32ThreadNum;

	  for (auto&& item : nConfig) {
		Ie.SetConfig(item.second, item.first);
	  }
	}

	// 2. Read IR Generated by ModelOptimizer (.xml and .bin files)
	std::string strBinFileName = m_strModelPath.substr(0, m_strModelPath.rfind('.')) + ".bin";
	LOG(INFO) << "Loading network files: \n\t" << m_strModelPath << "\n\t" << strBinFileName;
	m_Network = Ie.ReadNetwork(m_strModelPath);
	LOG(INFO) << "Batch size is " << std::to_string(m_Network.getBatchSize());

	// 3. Prepare input blobs
	LOG(INFO) << "Preparing input blobs";
	InferenceEngine::InputsDataMap InputInfo(m_Network.getInputsInfo());
	for (auto & item : InputInfo) {
	  auto input_data = item.second;
	  input_data->setPrecision(InferenceEngine::Precision::U8);
	  input_data->setLayout(InferenceEngine::Layout::NCHW);
	}
	m_InputInfo = InputInfo.begin()->second;
	m_strInputName = InputInfo.begin()->first;

	// 4. Prepare output blobs 
	LOG(INFO) << "Preparing output blobs";
	InferenceEngine::OutputsDataMap OutputInfo(m_Network.getOutputsInfo());
	m_OutputInfo = OutputInfo.begin()->second;
	m_strOutputName = m_OutputInfo->getName();
	m_OutputInfo->setPrecision(InferenceEngine::Precision::FP32);
	CHECK_NOTNULL(m_OutputInfo);

	const InferenceEngine::SizeVector OutputDims = m_OutputInfo->getTensorDesc().getDims();
	const size_t u32ObjectSize = OutputDims[3];

	if (u32ObjectSize != 7) {
	  LOG(ERROR) << "Output item should have 7 as a last dimension";
	  return -1;
	}

	if (OutputDims.size() != 4) {
	  LOG(ERROR) << "Incorrect output dimensions for SSD model";
	  return -1;
	}

	// 5. Loading model to the device 	
	LOG(INFO) << "Loading model to the device";
	InferenceEngine::ExecutableNetwork ExecutableNetwork = Ie.LoadNetwork(
	  m_Network,
	  m_InferenceOptions.OpenvinoOptions.strDeviceType);

	// 6. Create infer request 
	LOG(INFO) << "Create infer request";
	m_InferRrequest = ExecutableNetwork.CreateInferRequest();

	LOG(INFO) << "End init.";
	return 0;
  }

  AIP_API int rmInferenceDetectionOpenvino::Detect(const cv::Mat& cvMatImage, std::vector<OBJECT_INFO_S>* pstnObject) {
	CHECK_NOTNULL(pstnObject);

	LOG(INFO) << "Start detect.";

	// 1. Prepare input
	int s32ImageWidth = cvMatImage.cols;
	int s32ImageHeight = cvMatImage.rows;

	// resize image
	cv::Mat cvMatImageResized(cvMatImage);
	cv::resize(cvMatImage, cvMatImageResized, cv::Size(static_cast<int>(m_InputInfo->getTensorDesc().getDims()[3]),
	  static_cast<int>(m_InputInfo->getTensorDesc().getDims()[2])));

	// Creating input blob 
	InferenceEngine::Blob::Ptr pstImageInput = m_InferRrequest.GetBlob(m_strInputName);
	// Filling input tensor with images. First b channel, then g and r channels 
	InferenceEngine::MemoryBlob::Ptr pstMemortImage = InferenceEngine::as<InferenceEngine::MemoryBlob>(pstImageInput);
	if (!pstMemortImage) {
	  LOG(ERROR) << "We expect image blob to be inherited from MemoryBlob, but by fact we were not able "
		"to cast imageInput to MemoryBlob";
	  return -1;
	}
	// locked memory holder should be alive all time while access to its buffer happens
	auto InputHolder = pstMemortImage->wmap();
	size_t u32ChannelsNum = pstMemortImage->getTensorDesc().getDims()[1];
	size_t u32ImageSize = pstMemortImage->getTensorDesc().getDims()[3] * pstMemortImage->getTensorDesc().getDims()[2];
	unsigned char *pstucData = InputHolder.as<unsigned char *>();

	/** Iterate over all pixel in image (b,g,r) **/
	for (size_t u32Pid = 0; u32Pid < u32ImageSize; u32Pid++) {
	  /** Iterate over all channels **/
	  for (size_t u32Ch = 0; u32Ch < u32ChannelsNum; ++u32Ch) {
		/**          [images stride + channels stride + pixel id ] all in bytes            **/
		pstucData[u32Ch * u32ImageSize + u32Pid] = cvMatImageResized.data[u32Pid * u32ChannelsNum + u32Ch];
	  }
	}

	// 2. Do inference 	
	LOG(INFO) << "Start inference";
	m_InferRrequest.Infer();

	// 3. Process output
	LOG(INFO) << "Processing output blobs";
	const InferenceEngine::Blob::Ptr pstOutputBlob = m_InferRrequest.GetBlob(m_strOutputName);
	InferenceEngine::MemoryBlob::CPtr pstMemoryOutput = InferenceEngine::as<InferenceEngine::MemoryBlob>(pstOutputBlob);
	if (!pstMemoryOutput) {
	  LOG(ERROR) << "We expect output to be inherited from MemoryBlob, "
		"but by fact we were not able to cast output to MemoryBlob";
	  return -1;
	}

	// locked memory holder should be alive all time while access to its buffer happens
	auto OutputHolder = pstMemoryOutput->rmap();
	const float *pstf32Detection = OutputHolder.as<const InferenceEngine::PrecisionTrait<InferenceEngine::Precision::FP32>::value_type *>();

	std::vector<std::vector<int> > nBox(m_Network.getBatchSize());
	std::vector<std::vector<int> > nClasse(m_Network.getBatchSize());

	const InferenceEngine::SizeVector nOutputDim = m_OutputInfo->getTensorDesc().getDims();
	const size_t u32MaxProposalCount = nOutputDim[2];
	const size_t u32ObjectSize = nOutputDim[3];

	/* Each detection has image_id that denotes processed image */
	for (int s32CurProposal = 0; s32CurProposal < u32MaxProposalCount; s32CurProposal++) {

	  float f32Confidence = pstf32Detection[s32CurProposal * u32ObjectSize + 2];
	  auto label = static_cast<int>(pstf32Detection[s32CurProposal * u32ObjectSize + 1]);
	  auto xmin = static_cast<int>(pstf32Detection[s32CurProposal * u32ObjectSize + 3] * s32ImageWidth);
		xmin = std::max(0, xmin);
		xmin = std::min(xmin, s32ImageWidth - 1);
	  auto ymin = static_cast<int>(pstf32Detection[s32CurProposal * u32ObjectSize + 4] * s32ImageHeight);
		ymin = std::max(0, ymin);	
		ymin = std::min(ymin, s32ImageHeight - 1);
	  auto xmax = static_cast<int>(pstf32Detection[s32CurProposal * u32ObjectSize + 5] * s32ImageWidth);
		xmax = std::max(0, xmax);
		xmax = std::min(xmax, s32ImageWidth - 1);
	  auto ymax = static_cast<int>(pstf32Detection[s32CurProposal * u32ObjectSize + 6] * s32ImageHeight);
		ymax = std::max(0, ymax);	
		ymax = std::min(ymax, s32ImageHeight - 1);

	  if (f32Confidence > m_InferenceOptions.f64Threshold) {
		/** Drawing only objects with >50% probability **/
		OBJECT_INFO_S Object;
		Object.strClassName = m_InferenceOptions.nClassName[label];
		Object.f32Score = f32Confidence;
		Object.cvRectLocation.x = xmin;
		Object.cvRectLocation.y = ymin;
		Object.cvRectLocation.width = xmax - xmin;
		Object.cvRectLocation.height = ymax - ymin;
		pstnObject->push_back(Object);
	  }
	}

	LOG(INFO) << "end detect.";
	return 0;
  }

}